name: "Old Local Integration Tests"

env:
  TEST_USERNAME: "test-user"
  TEST_PASSWORD: "P@sswo3d"
  TEST_SUPERADMIN_USER: "test-user-2"
  TEST_SUPERADMIN_PASSWORD: "P@sswo3d-2"
  NEBARI_IMAGE_TAG: "main"
  PYTHON_VERSION: "3.11"

on:
  pull_request:
    paths:
      - ".github/workflows/test_local_integration.yaml"
      - "tests/**"
      - "scripts/**"
      - "src/**"
      - "pyproject.toml"
      - "pytest.ini"
      - ".cirun.yml"
  push:
    branches:
      - main
      - release/\d{4}.\d{1,2}.\d{1,2}
    paths:
      - ".github/workflows/test_local_integration.yaml"
      - "tests/**"
      - "scripts/**"
      - "src/**"
      - "pyproject.toml"
      - "pytest.ini"
      - ".cirun.yml"
  workflow_call:
    inputs:
      pr_number:
        required: true
        type: string
  workflow_dispatch:

# When the cancel-in-progress: true option is specified, any concurrent jobs or workflows using the same
# concurrency group will cancel both the pending and currently running jobs or workflows. This allows only
# one job or workflow in the concurrency group to be in progress at a time.
# concurrency:
  # group: ${{ github.workflow }}-${{ github.head_ref || github.run_id }}
  # cancel-in-progress: true

jobs:
  test-local-integration-old:
    runs-on: "cirun-runner--${{ github.run_id }}"
    defaults:
      run:
        shell: bash -l {0}
    steps:
      - name: "Checkout Infrastructure"
      # Pin checkout version due to issue with node24 support on self-hosted runners
        uses: actions/checkout@v4

      - name: "Checkout Nebari repository"
        uses: actions/checkout@v4
        with:
          repository: nebari-dev/nebari
          ref: main
          path: nebari

      # https://kind.sigs.k8s.io/docs/user/known-issues/#pod-errors-due-to-too-many-open-files
      - name: "Update inotify ulimit"
        run: |
          sudo sysctl fs.inotify.max_user_watches=524288
          sudo sysctl fs.inotify.max_user_instances=512

      - name: Setup runner for local deployment
        uses: ./.github/actions/setup-local

      - name: Checkout the branch from the PR that triggered the job
        if: ${{ github.event_name == 'issue_comment' }}
        run: |
          hub version
          hub pr checkout ${{ inputs.pr_number }}
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      - name: Set up Python
        uses: conda-incubator/setup-miniconda@v3
        env:
          CONDA: /home/runnerx/miniconda3
        with:
          auto-update-conda: true
          python-version: ${{ env.PYTHON_VERSION }}
          miniconda-version: "latest"
          activate-environment: nebari

      - name: Install JQ
        run: |
          # Wait for apt locks with timeout
          timeout=15
          elapsed=0
          while [ $elapsed -lt $timeout ]; do
            if ! sudo fuser /var/lib/dpkg/lock-frontend >/dev/null 2>&1 && \
               ! sudo fuser /var/lib/apt/lists/lock >/dev/null 2>&1 && \
               ! sudo fuser /var/lib/dpkg/lock >/dev/null 2>&1; then
              break
            fi
            echo "Waiting for apt locks to be released... (${elapsed}s)"
            sleep 1
            elapsed=$((elapsed + 1))
          done

          if [ $elapsed -ge $timeout ]; then
            echo "Timeout waiting for apt locks"
            exit 1
          fi

          sudo apt-get update
          sudo apt-get install jq -y

      - name: Install Nebari and playwright
        run: |
          pip install -e .[dev]          
          pip install ./nebari[dev]  
          playwright install

          conda env export | tee conda-env.yaml

      - name: Save conda lockfile artifacts
        if: always()
        uses: actions/upload-artifact@v4.3.1
        with:
          name: conda-env-spec
          path: |
            conda-env.yaml

      - name: Initialize Nebari config for local deployment
        id: init
        uses: ./.github/actions/init-local

      - name: Deploy Nebari
        working-directory: ${{ steps.init.outputs.directory }}
        run: nebari deploy --config ${{ steps.init.outputs.config }} --disable-prompt

      - name: Health check
        uses: ./.github/actions/health-check
        with:
          domain: ${{ steps.init.outputs.domain }}

      - name: Create example-users
        working-directory: ${{ steps.init.outputs.directory }}
        run: |
          nebari keycloak add-user --user "${TEST_USERNAME}" -p "${TEST_PASSWORD}" --config ${{ steps.init.outputs.config }}
          nebari keycloak add-user --user "${TEST_SUPERADMIN_USER}" -p "${TEST_SUPERADMIN_PASSWORD}" --config ${{ steps.init.outputs.config }} --groups superadmin
          nebari keycloak list-users --config ${{ steps.init.outputs.config }}

      - name: Await Workloads
        uses: jupyterhub/action-k8s-await-workloads@v3
        with:
          workloads: "" # all
          namespace: "dev"
          timeout: 300
          max-restarts: 3

      ### DEPLOYMENT TESTS
      - name: Deployment Pytests
        env:
          NEBARI_CONFIG_PATH: ${{ steps.init.outputs.config }}
          KEYCLOAK_USERNAME: ${{ env.TEST_USERNAME }}
          KEYCLOAK_PASSWORD: ${{ env.TEST_PASSWORD }}
        run: |
          pytest nebari/tests/tests_deployment/ -v -s

      ### USER-JOURNEY TESTS
      - uses: actions/setup-node@v4
        with:
          node-version: 20

      - name: Playwright Tests
        env:
          KEYCLOAK_USERNAME: ${{ env.TEST_USERNAME }}
          KEYCLOAK_PASSWORD: ${{ env.TEST_PASSWORD }}
          NEBARI_FULL_URL: "https://${{ steps.init.outputs.domain }}/"
        working-directory: nebari/tests/tests_e2e/playwright
        run: |
          # create environment file
          envsubst < .env.tpl > .env
          # run playwright pytest tests in headed mode with the chromium browser
          xvfb-run pytest --browser chromium --slowmo 300 --headed

      - name: Save Playwright recording artifacts
        if: always()
        uses: actions/upload-artifact@v4.3.1
        with:
          name: e2e-playwright
          path: |
            ./nebari/tests/tests_e2e/playwright/videos/

      ### CONDA-STORE TESTS
      - name: "Checkout conda-store"
        # Pin checkout version due to issue with node24 support on self-hosted runners
        uses: actions/checkout@v4
        with:
          repository: conda-incubator/conda-store
          ref: main
          path: conda-store

      - name: "Set up conda-store conda env"
        uses: conda-incubator/setup-miniconda@v3
        env:
          CONDA: /home/runnerx/miniconda3
        with:
            environment-file: conda-store/conda-store-server/environment-dev.yaml
            miniforge-version: latest
            auto-activate-base: false
            activate-environment: conda-store-server-dev
            python-version: ${{ env.PYTHON_VERSION }}
            conda-remove-defaults: "true"

      - name: Install conda-store dependencies
        run: |
          python -m pip install "conda-lock>=1.0.5,<3.0.0"  # Added until https://github.com/conda-incubator/conda-store/issues/1095 is solved upstream
          # Below added until https://github.com/nebari-dev/nebari/issues/3118 is resolved upstream
          conda install constructor==3.11.3
          python -m pip install conda-store/conda-store-server

      - name: Get conda store token from login
        id: conda-store-token
        env:
          BASE_URL: ${{ steps.init.outputs.domain }}
          KEYCLOAK_USERNAME: ${{ env.TEST_SUPERADMIN_USER }}
          KEYCLOAK_PASSWORD: ${{ env.TEST_SUPERADMIN_PASSWORD }}
        run: |
          echo "CONDA_STORE_TOKEN=$(python scripts/get_conda_store_token.py)" >> "$GITHUB_OUTPUT"

      - name: Run conda-store-server user_journey tests
        env:
          CONDA_STORE_BASE_URL: https://${{ steps.init.outputs.domain }}
          CONDA_STORE_TEST_VERIFY_SSL: 0
          CONDA_STORE_TOKEN: ${{ steps.conda-store-token.outputs.CONDA_STORE_TOKEN }}
        run: |
          curl --insecure --header "Authorization: Bearer ${CONDA_STORE_TOKEN}" ${CONDA_STORE_BASE_URL}/conda-store/api/v1/permission/ | jq
          cd conda-store/conda-store-server
          python -m pytest -m "user_journey"

      - name: Get conda-store-server logs
        if: ${{ failure() }}
        run: |
          kubectl logs -n dev deployment/nebari-conda-store-server

      ### CLEANUP AFTER TESTS
      - name: Cleanup nebari deployment
        # Since this is not critical for most pull requests and takes more than half of the time
        # in the CI, it makes sense to only run on merge to main or workflow_dispatch to speed
        # up feedback cycle
        if: github.ref_name == 'main' || github.event_name == 'workflow_dispatch'
        working-directory: ${{ steps.init.outputs.directory }}
        # We need to use the conda environment to run the nebari command as the current environment
        # is the one that was created to run the conda-store user journey tests
        run: conda run -n nebari nebari destroy --config ${{ steps.init.outputs.config }} --disable-prompt
